{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Setup"
      ],
      "metadata": {
        "id": "20TkQJgHgar7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(num_authors = 99):\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  data_root_dir = \"../data/corpora/amt/\"\n",
        "  authors_to_ignore = []\n",
        "  authorCount = 0\n",
        "\n",
        "  for author_name in os.listdir(data_root_dir):\n",
        "      # Check if the maximum number of authors has been parsed\n",
        "      if authorCount > self.numAuthors:\n",
        "         break\n",
        "\n",
        "      if author_name not in authors_to_ignore:\n",
        "         label = author_name\n",
        "         documents_path = data_root_dir + author_name + \"/\"\n",
        "         authorCount += 1\n",
        "\n",
        "         for doc in os.listdir(documents_path):\n",
        "            if validate_file(doc):\n",
        "              text = open(docPath + doc, errors = \"ignore\").read()\n",
        "              X.append(text)\n",
        "              y.append(label)\n",
        "\n",
        "  return X, y\n"
      ],
      "metadata": {
        "id": "Ww6iD5kRWITq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_file(file_name):\n",
        "    filterWords = [\"imitation\", \"demographics\", \"obfuscation\", \"verification\"]\n",
        "    for fw in filterWords:\n",
        "        if fw in file_name:\n",
        "            return False\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "DePJ-ociWKEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QV12JSmPgfj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "jgLz7TbvggLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library Imports"
      ],
      "metadata": {
        "id": "j1Jfoifcgil0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from sortedcontainers import SortedDict\n",
        "from keras.preprocessing import text\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "CfI3TU-1WEte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Characters"
      ],
      "metadata": {
        "id": "-o-zBT1CgnOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CountChars(input):\n",
        "    num_chars = len(input)\n",
        "    return num_chars\n"
      ],
      "metadata": {
        "id": "JCdFyUJRWQJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average Chars per Word"
      ],
      "metadata": {
        "id": "hNc8Tg3OgpzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def averageCharacterPerWord(input):\n",
        "    text_array = text.text_to_word_sequence(input, \n",
        "                                            filters=' !#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n\"', \n",
        "                                            lower=False, split=\" \")\n",
        "    num_words = len(text_array)\n",
        "\n",
        "    text_without_spaces = input.replace(\" \", \"\")\n",
        "    num_chars = len(text_without_spaces)\n",
        "\n",
        "    avgCharPerWord = 1.0 * num_chars / num_words\n",
        "    return avgCharPerWord\n"
      ],
      "metadata": {
        "id": "4ToLki0PXBZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alphabet Frequency"
      ],
      "metadata": {
        "id": "clE026C8gszP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequencyOfLetters(input):\n",
        "    input = input.lower()  # because its case sensitive\n",
        "    input = input.lower().replace(\" \", \"\")\n",
        "    num_chars = len(input)\n",
        "\n",
        "    characters = \"abcdefghijklmnopqrstuvwxyz\".split()\n",
        "    frequencies = []\n",
        "\n",
        "    for each_char in characters:\n",
        "      char_count = input.count(each_char)\n",
        "      if char_count < 0:\n",
        "        frequencies.append(0)\n",
        "      else:\n",
        "        frequencies.append(char_count/num_chars)\n",
        "\n",
        "    return frequencies\n"
      ],
      "metadata": {
        "id": "pD9ZGZrfXDZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Bigrams"
      ],
      "metadata": {
        "id": "ew8ZicThhCzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CommonLetterBigramFrequency(input):\n",
        "\n",
        "    common_bigrams = ['th','he','in','er','an','re','nd',\n",
        "                      'at','on','nt','ha','es','st','en',\n",
        "                      'ed','to','it','ou','ea','hi','is',\n",
        "                      'or','ti','as','te','et','ng','of',\n",
        "                      'al','de','se','le','sa','si','ar',\n",
        "                      've','ra','ld','ur']\n",
        "    bigramCounter = []\n",
        "    \n",
        "    input = input.lower().replace(\" \", \"\")\n",
        "\n",
        "    for bigram in common_bigrams:\n",
        "      bigram_count = input.count(bigram)\n",
        "      if bigram_count == -1:\n",
        "        bigramCounter.append(0)\n",
        "      else:\n",
        "        bigramCounter.append(bigram_count)\n",
        "\n",
        "    total_bigram_count = np.sum(bigramCounter)\n",
        "    bigramCounterNormalized = []\n",
        "    for bigram_count in bigramCounter:\n",
        "      bigramCounterNormalized.append(bigram_count / total_bigram_count)\n",
        "\n",
        "    return bigramCounterNormalized"
      ],
      "metadata": {
        "id": "0gXF-X7xXE8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Trigrams"
      ],
      "metadata": {
        "id": "DnvLy9iOhHmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CommonLetterTrigramFrequency(input):\n",
        "\n",
        "    common_trigrams = [\"the\", \"and\", \"ing\", \"her\", \"hat\", \n",
        "                       \"his\", \"tha\", \"ere\", \"for\", \"ent\", \n",
        "                       \"ion\", \"ter\", \"was\", \"you\", \"ith\",\n",
        "                       \"ver\", \"all\", \"wit\", \"thi\", \"tio\"]\n",
        "    trigramCounter = []\n",
        "    \n",
        "    input = input.lower().replace(\" \", \"\")\n",
        "\n",
        "    for trigram in common_trigrams:\n",
        "      trigram_count = input.count(trigram)\n",
        "      if trigram_count == -1:\n",
        "        trigramCounter.append(0)\n",
        "      else:\n",
        "        trigramCounter.append(trigram_count)\n",
        "\n",
        "    total_trigram_count = np.sum(trigramCounter)\n",
        "    trigramCounterNormalized = []\n",
        "    for trigram_count in trigramCounter:\n",
        "      trigramCounterNormalized.append(trigram_count / total_trigram_count)\n",
        "\n",
        "    return trigramCounterNormalized\n"
      ],
      "metadata": {
        "id": "v8QurRMZ4cm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Percentage Digits"
      ],
      "metadata": {
        "id": "QmZ6Wrl9hKqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def digitsPercentage(input):\n",
        "\n",
        "    num_chars = len(input)\n",
        "    num_digits = 0\n",
        "\n",
        "    for each_char in input:\n",
        "      if each_char.isnumeric():\n",
        "        num_digits = num_digits + 1\n",
        "    \n",
        "    digit_percent = num_digits / num_chars\n",
        "    return digit_percent\n"
      ],
      "metadata": {
        "id": "c_duyCQDXPht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Percentage Alphabets"
      ],
      "metadata": {
        "id": "TAsX7jNuhOeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def charactersPercentage(input):\n",
        "\n",
        "    input = input.lower().replace(\" \", \"\")\n",
        "    characters = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "    total_chars = len(input)\n",
        "    char_count = 0\n",
        "\n",
        "    for each_char in input:\n",
        "      if each_char in characters:\n",
        "        char_count = char_count + 1\n",
        "    \n",
        "    char_percent = char_count / total_chars\n",
        "    return char_percent\n"
      ],
      "metadata": {
        "id": "5KbKgWsiXSxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Digits Frequency"
      ],
      "metadata": {
        "id": "4scKgjrbhWah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequencyOfDigits(input):\n",
        "\n",
        "    input = input.lower().replace(\" \", \"\")\n",
        "    num_chars = len(input)\n",
        "\n",
        "    digits = \"0123456789\".split()\n",
        "    frequencies = []\n",
        "\n",
        "    for each_digit in digits:\n",
        "      digit_count = input.count(each_digit)\n",
        "      if digit_count < 0:\n",
        "        frequencies.append(0)\n",
        "      else:\n",
        "        frequencies.append(digit_count/num_chars)\n",
        "\n",
        "    return frequencies\n"
      ],
      "metadata": {
        "id": "eueU1YT7XVzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Percentage Upper Case Chars"
      ],
      "metadata": {
        "id": "a2Nq0YYShYo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upperCaseCharactersPercentage(input):\n",
        "\n",
        "    input = input.replace(\" \", \"\")\n",
        "    upper_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "    num_chars = len(input)\n",
        "    upper_count = 0\n",
        "\n",
        "    for each_char in upper_characters:\n",
        "      char_count = input.count(each_char)\n",
        "      if char_count > 0:\n",
        "        upper_count = upper_count + char_count\n",
        "        \n",
        "    upper_percent = upper_count / num_chars\n",
        "    return upper_percent\n"
      ],
      "metadata": {
        "id": "mHr2ITZhXcPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special Character Frequency"
      ],
      "metadata": {
        "id": "QR6FfqcKhcBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequencyOfSpecialCharacters(input):\n",
        "\n",
        "    SPECIAL_CHARS_FILE = \"static_files/writeprints_special_chars.txt\"\n",
        "    num_chars = len(input)\n",
        "    special_counts = []\n",
        "\n",
        "    special_characters = open(SPECIAL_CHARS_FILE , \"r\").readlines()\n",
        "    for each_char in special_characters:\n",
        "      special = each_char.strip().rstrip()\n",
        "      special_count = input.count(special)\n",
        "      if special_count < 0:\n",
        "        special_counts.append(0)\n",
        "      else:\n",
        "        special_counts.append(special_count / num_chars)\n",
        "\n",
        "    return special_counts\n"
      ],
      "metadata": {
        "id": "Xa7udCFeXdF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Short Words"
      ],
      "metadata": {
        "id": "OqBC9as8hf8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CountShortWords(input):\n",
        "    words = text.text_to_word_sequence(input, filters=\",.?!\\\"'`;:-()&$\", \n",
        "                                       lower=True, split=\" \")\n",
        "    short_word_count = 0\n",
        "\n",
        "    for word in words:\n",
        "        if len(word) <= 3:\n",
        "            short_word_count = short_word_count + 1\n",
        "\n",
        "    return short_word_count\n"
      ],
      "metadata": {
        "id": "5V1Qqa1hXgge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Count"
      ],
      "metadata": {
        "id": "cxXxDhLkhiQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CountWords(input):\n",
        "    words = text.text_to_word_sequence(input, filters=\",.?!\\\"'`;:-()&$\", \n",
        "                                       lower=True, split=\" \")\n",
        "    return len(words)\n"
      ],
      "metadata": {
        "id": "Mbj8ZtQLXjaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average Word Length"
      ],
      "metadata": {
        "id": "4ywexj1xhksq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def averageWordLength(input):\n",
        "    words = text.text_to_word_sequence(inputText, filters=\",.?!\\\"'`;:-()&$\", \n",
        "                                       lower=True, split=\" \")\n",
        "    lengths = []\n",
        "    for word in words:\n",
        "        lengths.append(len(word))\n",
        "    return np.mean(lengths)\n"
      ],
      "metadata": {
        "id": "YZwwwlIsXmow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it Together"
      ],
      "metadata": {
        "id": "iDn46ZBshoHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_features(input):\n",
        "  \n",
        "  features = []\n",
        "\n",
        "  features.extend([CountWords(input)])\n",
        "  features.extend([averageWordLength(input)])\n",
        "  features.extend([CountShortWords(input)])\n",
        "  features.extend([CountChars(input)])\n",
        "  features.extend([averageCharacterPerWord(input)])\n",
        "  features.extend([frequencyOfLetters(input)])\n",
        "  features.extend([CommonLetterBigramFrequency(input)])\n",
        "  features.extend([CommonLetterTrigramFrequency(input)])\n",
        "  features.extend([digitsPercentage(input)])\n",
        "  features.extend([charactersPercentage(input)])\n",
        "  features.extend([frequencyOfDigits(input)])\n",
        "  features.extend([upperCaseCharactersPercentage(input)])\n",
        "  features.extend([frequencyOfSpecialCharacters(input)])\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "1WHrD0paXpin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_original, Y = read_dataset(num_authors = 6)\n",
        "X_Features = []\n",
        "for x in X_original:\n",
        "  x_features = calculate_features(x)\n",
        "  X.append(x_features)\n"
      ],
      "metadata": {
        "id": "sWHi2mzGX8sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authorship Attributors"
      ],
      "metadata": {
        "id": "hO2brlrZhs4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Training and Test Datasets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_Features, Y)\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier(n_estimators = 100) \n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "Y_predicted = model.predict(X_test)\n",
        "confusion = confusion_matrix(Y_test, Y_predicted)\n",
        "plt.figure(figsize = (10,8))\n",
        "sns.heatmap(confusion, annot = True, \n",
        "            fmt = 'd', cmap=\"YlGnBu\")\n"
      ],
      "metadata": {
        "id": "CXHVaLDIYLeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(actual, predicted):\n",
        "  total_examples = len(actual)\n",
        "  correct_examples = 0\n",
        "\n",
        "  for idx in range(total_examples):\n",
        "    if actual[i] == predicted[i]:\n",
        "      correct_examples = correct_examples + 1\n",
        "  \n",
        "  accuracy = correct_examples / total_examples\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "R_58UOLfYRzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report \n",
        "classification_report(Y_test, Y_predicted)\n"
      ],
      "metadata": {
        "id": "bHOK-RxqYXgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authorship Obfuscation"
      ],
      "metadata": {
        "id": "85g_zHyJhy70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import WSD_with_UKB as wsd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n"
      ],
      "metadata": {
        "id": "cmmAtB6MYd0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contraction Replacement"
      ],
      "metadata": {
        "id": "HEY6qScAh2ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contraction_replacement(sentence):\n",
        "\n",
        "    # Read Contractions\n",
        "    CONTRACTION_FILE = 'contraction_extraction.pickle'\n",
        "    with open(CONTRACTION_FILE, 'rb') as contraction_file:\n",
        "        contractions = pickle.load(contraction_file)\n",
        "\n",
        "    # Calculate contraction counts\n",
        "    all_contractions = contractions.keys()\n",
        "    contractions_count = 0\n",
        "    for contraction in all_contractions:\n",
        "        if contraction.lower() in sentence.lower():\n",
        "            contractions_count += 1\n",
        "\n",
        "    # Calculate expansion counts        \n",
        "    all_expansions = contractions.values()\n",
        "    expansions_count = 0\n",
        "    for expansion in all_expansions:\n",
        "        if expansion.lower() in sentence.lower():\n",
        "            expansions_count += 1\n",
        "\n",
        "    if contractions_count > expansions_count:\n",
        "        # There are more contractions than expansions\n",
        "        # So we should replace all contractions with their expansions\n",
        "        temp_contractions = dict((k.lower(), v) for k, v in contractions.items())\n",
        "        for contraction in all_contractions:\n",
        "            if contraction.lower() in sentence.lower():\n",
        "                case_insesitive = re.compile(re.escape(contraction.lower()), re.IGNORECASE)\n",
        "                sentence = case_insesitive.sub(temp_contractions[contraction.lower()], sentence)\n",
        "        contractions_applied = True\n",
        "\n",
        "    elif expansions_count > contractions_count:\n",
        "        # There are more expansions than contractions \n",
        "        # So we should replace expansions by contractions\n",
        "        inv_map = {v: k for k, v in contractions.items()}\n",
        "        temp_contractions = dict((k.lower(), v) for k, v in inv_map.items())\n",
        "        for expansion in all_expansions:\n",
        "            if expansion.lower() in sentence.lower():\n",
        "                case_insesitive = re.compile(re.escape(expansion.lower()), re.IGNORECASE)\n",
        "                sentence = case_insesitive.sub(temp_contractions[expansion.lower()], sentence)\n",
        "        contractions_applied = True\n",
        "\n",
        "    else:\n",
        "        # Both expansions and contractions are equal\n",
        "        # So do nothing \n",
        "        contractions_applied = False\n",
        "\n",
        "    return sentence, contractions_applied\n"
      ],
      "metadata": {
        "id": "VlKEHo-XYe2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parantheses Removal"
      ],
      "metadata": {
        "id": "XF4x5VsTh5Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_parenthesis(sentence):\n",
        "    parantheses = ['(', ')', '{', '}', '[', ']']\n",
        "    for paranthesis in parantheses:\n",
        "      sentence = sentence.replace(paranthesis, \"\")\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "d6jiV2LGYoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discourse Marker Removal"
      ],
      "metadata": {
        "id": "CR5sO6Abh7z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_discourse_markers(sentence):\n",
        "\n",
        "    # Read Discourse Markers\n",
        "    DISCOURSE_FILE = 'discourse_markers.pkl'\n",
        "    with open(DISCOURSE_FILE , 'rb') as discourse_file:\n",
        "        discourse_markers = pickle.load(discourse_file)\n",
        "\n",
        "    sent_tokens = sentence.lower().split()\n",
        "    for marker in discourse_markers:\n",
        "        if marker.lower() in sent_tokens:\n",
        "            case_insensitive = re.compile(re.escape(marker.lower()), re.IGNORECASE)\n",
        "            sentence = case_insensitive.sub('', sentence)\n",
        "\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "h8ZgmVvIYrvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apposition Removal"
      ],
      "metadata": {
        "id": "6Brq1JVPh_hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_appositions(sentence):\n",
        "    sentence = re.sub(r\" ?\\,[^)]+\\,\", \"\", sentence)\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "9hgs2klAYvct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Possesive Transformation"
      ],
      "metadata": {
        "id": "10BycHgOiy_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_possessive_transformation(text):\n",
        "    if re.match(r\"(\\w+) of (\\w+)\", text):\n",
        "        rnd = random.choice([False, True, False])\n",
        "        if rnd:\n",
        "            return re.sub(r\"(\\w+) of (\\w+)\" , r\"\\2's \\1\", text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "90z3r9QIY0V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Equation Transformation"
      ],
      "metadata": {
        "id": "pv-Mgbi1i63T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_equation_transformation(text):\n",
        "    words = RegexpTokenizer(r'\\w+').tokenize(text)\n",
        "    symbol_to_text =   {\n",
        "                '+': ' plus ',\n",
        "                '-': ' minus ',\n",
        "                '*': ' multiplied by ',\n",
        "                '/': ' divided by ',\n",
        "                '=': ' equals ',\n",
        "                '>': ' greather than ',\n",
        "                '<': ' less than ',\n",
        "                '<=': ' less than or equal to ',\n",
        "                '>=': ' greater than or equal to ',\n",
        "            }\n",
        "    for n,w in enumerate(words):\n",
        "        for symbol in symbol_to_text:\n",
        "            if symbol in w:\n",
        "                words[n] = words[n].replace(symbol, symbol_to_text[sym])\n",
        "\n",
        "    sentence = ''\n",
        "    for word in words:\n",
        "      sentence = sentence + word + \" \"\n",
        "\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "1FFiOqzMY4IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Untokenization"
      ],
      "metadata": {
        "id": "_cEfVd7Ai_6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def untokenize(words):\n",
        "    text = ' '.join(words)\n",
        "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n",
        "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
        "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
        "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
        "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
        "        \"can not\", \"cannot\")\n",
        "    step6 = step5.replace(\" ` \", \" '\")\n",
        "    return step6.strip()\n"
      ],
      "metadata": {
        "id": "nseBaSBHY8oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synonym Substitution"
      ],
      "metadata": {
        "id": "g9aXQthLjCc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synonym_substitution(sentence, all_words):\n",
        "    new_tokens = []\n",
        "    output = wsd.process_text(sentence)\n",
        "\n",
        "    for token, synset in output:\n",
        "        if synset != None:\n",
        "            try:\n",
        "                # Get the synset name\n",
        "                synset = synset.split('-')\n",
        "                offset = int(synset[0])\n",
        "                pos = synset[1]\n",
        "                synset_name = wn.synset_from_pos_and_offset(pos, offset)\n",
        "\n",
        "                # List of Synonyms\n",
        "                synonyms = synset_name.lemma_names()\n",
        "\n",
        "                for synonym in synonyms:\n",
        "                    if synonym.lower() not in all_words:\n",
        "                        token = synonym\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                # Some error in the synset naming....\n",
        "                continue\n",
        "\n",
        "        new_tokens.append(token)\n",
        "\n",
        "    final = untokenize(new_tokens)\n",
        "    final = final.capitalize()\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "yF_HjYkcZC59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it Together"
      ],
      "metadata": {
        "id": "N6R9f1uijPtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obfuscate_text(input_text):\n",
        "    obfuscated_text = []\n",
        "    sentences = sent_tokenize(input_text)\n",
        "    tokens = set(nltk.word_tokenize(input_text.lower()))\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # 1. Apply Contractions\n",
        "        sentence, contractions_applied = contraction_replacement(sentence, contractions)\n",
        "\n",
        "        # 2. Remove Parantheses\n",
        "        sentence = remove_parenthesis(sentence)\n",
        "\n",
        "        # 3. Remove Discourse Markers\n",
        "        sentence = remove_discourse_markers(sentence, discourse_markers)\n",
        "\n",
        "        # 4. Remove Appositions\n",
        "        sentence = remove_appositions(sentence)\n",
        "\n",
        "        # 5. Synonym Substitution\n",
        "        sentence = synonym_substitution(sentence, tokens)\n",
        "\n",
        "        # 6. Apply possessive transformation\n",
        "        sentence = apply_possessive_transformation(sentence)\n",
        "\n",
        "        # 7. Apply equation transformation\n",
        "        sentence = apply_equation_transformation(sentence)\n",
        "\n",
        "        obfuscated_text.append(sentence)\n",
        "\n",
        "    obfuscated_text = \" \".join(obfuscated_text)\n",
        "    return obfuscated_text\n"
      ],
      "metadata": {
        "id": "I0GtR46EZHw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Obfuscation"
      ],
      "metadata": {
        "id": "j7_M3gkejTOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# Read Data\n",
        "X, Y = read_dataset(num_authors = 6)\n",
        "\n",
        "# Split it into train and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
        "\n",
        "# Extract features from training data\n",
        "X_train_features = []\n",
        "for x in X_train:\n",
        "  x_features = calculate_features(x)\n",
        "  X_train_features.append(x_features)\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier(n_estimators = 100) \n",
        "model.fit(X_train_features, Y_train)\n",
        "\n",
        "X_test_obfuscated = []\n",
        "for x in X_test:\n",
        "  # Obfuscate\n",
        "  x_obfuscated = obfuscate_text(x)\n",
        "  # Extract features\n",
        "  x_obfuscated_features = calculate_features(x_obfuscated)\n",
        "  \n",
        "  X_test_obfuscated.append(x_obfuscated_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "hFljR2qxZLXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy on original\n",
        "Y_pred_original = model.predict(X_test)\n",
        "accuracy_orig = calculate_accuracy(Y_test, Y_pred_original)\n",
        "\n",
        "# Calculate accuracy on obfuscated\n",
        "Y_pred_obfuscated = model.predict(X_test_obfuscated)\n",
        "accuracy_obf = calculate_accuracy(Y_test, Y_pred_obfuscated)\n"
      ],
      "metadata": {
        "id": "tvjCd9LGZTxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}