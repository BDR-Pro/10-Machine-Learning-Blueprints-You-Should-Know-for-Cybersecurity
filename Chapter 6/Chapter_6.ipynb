{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "2oNuOguN3kjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lsK_GXsupgo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os \n",
        "from newspaper import Article"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Creation"
      ],
      "metadata": {
        "id": "yQAo_fYU3nwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scraping Real News"
      ],
      "metadata": {
        "id": "VPbRfENm3tvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/UCI-News-Aggregator-Classifier/data/uci-news-aggregator.csv\"\n",
        "df = pd.read_csv(path)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "NLe-_IK3u4DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"CATEGORY\"].value_counts().plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "o_5NptHwu4-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"HOSTNAME\"].value_counts()[:20].plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "jwmgXg1nu5Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"./articles\"\n",
        "fake = os.path.join(root, \"fake\")\n",
        "real = os.path.join(root, \"real\")\n",
        "\n",
        "for dir in [root, real, fake]:\n",
        "  if not os.path.exists(dir):\n",
        "    os.mkdir(dir)\n"
      ],
      "metadata": {
        "id": "icFFVSIWu5Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.groupby('CATEGORY').apply(lambda x: x.sample(250))\n",
        "\n",
        "df2[\"CATEGORY\"].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "PFwo4quxu5HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL_LIST = df2[\"URL\"].tolist()\n",
        "TITLE_LIST = df2[\"TITLE\"].tolist()\n",
        "for id_url, article_url in enumerate(URL_LIST):\n",
        "  article = Article(article_url)\n",
        "  try:\n",
        "    # Download and parse article\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    text = article.text\n",
        "\n",
        "    # Save to file \n",
        "    filename = os.path.join(real, \"Article_{}.txt\".format(id_url))\n",
        "    article_title = TITLE_LIST[id_url]\n",
        "    with open(filename, \"w\") as text_file:\n",
        "      text_file.write(\" %s \\n %s\" % (article_title, text))\n",
        "\n",
        "  except:\n",
        "    print(\"Could not download the article at: {}\".format(article_url))\n"
      ],
      "metadata": {
        "id": "NjYdzJ5Tu5J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Fake News"
      ],
      "metadata": {
        "id": "N8LhYyTU31B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "GPT_MODEL_NAME='774M'\n",
        "\n",
        "if not os.path.exists('models/'+GPT_MODEL_NAME):\n",
        "    gpt2.download_gpt2(model_name=GPT_MODEL_NAME)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=GPT_MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "G2WgMfuTu5O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id_title, title in enumerate(TITLE_LIST):\n",
        "  article=gpt2.generate(sess,\n",
        "                              model_name=GPT_MODEL_NAME,\n",
        "                              prefix=title,\n",
        "                              length=500,\n",
        "                              temperature=0.8,\n",
        "                              top_p=0.9,\n",
        "                              nsamples=1,\n",
        "                              batch_size=1,\n",
        "                              return_as_list=True\n",
        "                              )[0]\n",
        "\n",
        "  filename = os.path.join(fake, \"Article_{}.txt\".format(id_url))\n",
        "  with open(filename, \"w\") as text_file:\n",
        "      text_file.write(\" %s \\n %s\" % (title, text))\n"
      ],
      "metadata": {
        "id": "Ox7ZE_B7u5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combined Dataset"
      ],
      "metadata": {
        "id": "3EIsNyWB34wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "for file in os.listdir(real):\n",
        "  try:\n",
        "    with open(file, \"r\") as article_file:\n",
        "      article = file.read()\n",
        "      X.append(article)\n",
        "      Y.append(0)\n",
        "  except:\n",
        "    print(\"Error reading: {}\".format(file))\n",
        "    continue\n",
        "    \n",
        "\n",
        "for file in os.listdir(fake):\n",
        "  try:\n",
        "    with open(file, \"r\") as article_file:\n",
        "      article = file.read()\n",
        "      X.append(article)\n",
        "      Y.append(1)\n",
        "  except:\n",
        "    print(\"Error reading: {}\".format(file))\n",
        "    continue    \n"
      ],
      "metadata": {
        "id": "kd-E9hGZu5Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "I9yP4bdX38Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FUNCTION_WORD_FILE = '../static/function_words.txt'\n",
        "with open(FUNCTION_WORD_FILE,'r') as fwf:\n",
        "  k = fwf.readlines()\n",
        "  func_words = [w.rstrip() for w in k]\n",
        "    \n",
        "  #There might be duplicates!\n",
        "  func_words = list(set(func_words))\n",
        "\n",
        "def calculate_function_words(text):\n",
        "  function_word_counter = 0\n",
        "  text_length = len(text.split(' '))\n",
        "  for word in func_words:\n",
        "    function_word_counter = function_word_counter + text.count(word)\n",
        "\n",
        "  if text_length == 0:\n",
        "    feature = 0\n",
        "  else:\n",
        "    feature = function_word_counter / total_length \n",
        "\n",
        "  return feature\n"
      ],
      "metadata": {
        "id": "cMxkedAtu5X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_punctuation(text):\n",
        "  punctuations = =[ k for k in string.punctuation]\n",
        "  punctuation_counter = 0\n",
        "  total_length = len(text.split())\n",
        "\n",
        "  for punc in punctuations:\n",
        "    punctuation_counter = punctuation_counter + text.count(punc)\n",
        "\n",
        "  if text_length == 0:\n",
        "    feature = 0\n",
        "  else:\n",
        "    feature = punctuation_counter / total_length \n",
        "\n",
        "  return feature\n"
      ],
      "metadata": {
        "id": "2M8u-0j5u5a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ari(text):\n",
        "  chars = len(text.split())\n",
        "  words = len(text.split(' '))\n",
        "  sentences = len(text.split('.'))\n",
        "\n",
        "  if words == 0 or sentences == 0:\n",
        "    feature = 0\n",
        "  else:\n",
        "    feature = 4.71* (chars / words) + 0.5* (words / sentences) - 21.43\n",
        "  \n",
        "  return feature\n"
      ],
      "metadata": {
        "id": "u03eKjkHu5d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Features = []\n",
        "for x in X: \n",
        "  feature_vector = []\n",
        "  feature_vector.append(calculate_function_words(x))\n",
        "  feature_vector.append(calculate_punctuation(x))\n",
        "  feature_vector.append(calculate_ari(x))\n",
        "\n",
        "  X_Features.append(feature_vector)\n"
      ],
      "metadata": {
        "id": "jlTOSx87y0i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function for Evaluation"
      ],
      "metadata": {
        "id": "nWrT7mHC3_SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def evaluate_model(actual, predicted):\n",
        "  confusion = confusion_matrix(actual, predicted)\n",
        "  tn, fp, fn, tp = confusion.ravel()\n",
        "\n",
        "  total = tp + fp + tn + fn\n",
        "\n",
        "  accuracy = 100 * (tp + tn) / total\n",
        "  if tp + fp != 0:\n",
        "    precision = tp / (tp + fp)\n",
        "  else:\n",
        "    precision = 0\n",
        "\n",
        "  if tp + fn != 0:\n",
        "    recall = tp / (tp + fn)\n",
        "  else:\n",
        "    recall = 0\n",
        "\n",
        "  if precision == 0 or recall == 0:\n",
        "    f1 = 0\n",
        "  else:\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "  evaluation = { 'accuracy': accuracy,\n",
        "                 'precision': precision,\n",
        "                 'recall': recall,\n",
        "                 'f1': f1}\n",
        "\n",
        "  return evaluation\n"
      ],
      "metadata": {
        "id": "M2l0i6bNy3MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "_u7und-u4GeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_Features, Y)\n"
      ],
      "metadata": {
        "id": "9S4J0bv9y7hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "ezGl7gqt4I2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "Y_predicted = model.predict(X_test)\n",
        "print(evaluate_model(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "c1ajw1emy-BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "_UX5blSN4LlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators = 100) \n",
        "model.fit(X_train, Y_train)\n",
        "Y_predicted = model.predict(X_test)\n",
        "print(evaluate_model(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "3nTh3PcfzANW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ],
      "metadata": {
        "id": "oQT8C4AJ4N2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "model = MLPClassifier(hidden_layer_sizes = (50, 25, 10), \n",
        "                      max_iter = 100,\n",
        "                      activation = 'relu',\n",
        "                      solver = 'adam',\n",
        "                      random_state = 123)\n",
        "model.fit(X_train, Y_train)\n",
        "Y_predicted = model.predict(X_test)\n",
        "print(evaluate_model(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "UKyhSqbFzSeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine"
      ],
      "metadata": {
        "id": "Swq_C5394Q9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "model = svm.SVC(kernel='linear')\n",
        "model.fit(X_train, Y_train)\n",
        "Y_predicted = model.predict(X_test)\n",
        "print(evaluate_model(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "A2LMzn51zTIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_idf = TfidfVectorizer()\n",
        "X_train_TFIDF = tf_idf.fit_transform(X_train)\n",
        "X_test_TFIDF = tf_idf.transform(X_test)\n"
      ],
      "metadata": {
        "id": "A1y9RmDFzZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "model = MLPClassifier(hidden_layer_sizes = (300, 200, 100), \n",
        "                      max_iter = 100,\n",
        "                      activation = 'relu',\n",
        "                      solver = 'adam',\n",
        "                      random_state = 123)\n",
        "model.fit(X_train_TFIDF, Y_train)\n",
        "Y_predicted = model.predict(X_test_TFIDF)\n",
        "print(evaluate_model(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "cx9etGLzzZMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "corpus = []\n",
        "for x in X_train:\n",
        "  # Split into sentences\n",
        "  sentences_tokens = nltk.sent_tokenize(x)\n",
        "  # Split each sentence into words\n",
        "  word_tokens = [nltk.word_tokenize(sent) for sent in sentences_tokens]\n",
        "  # Add to corpus\n",
        "  corpus = corpus + word_tokens\n"
      ],
      "metadata": {
        "id": "q-TGFvI2zZKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(corpus, min_count=1, vector_size = 30)\n"
      ],
      "metadata": {
        "id": "YQvujMdozZHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vector_mean = []\n",
        "for x in X_train:\n",
        "  # Create a 30-element vector with all zeroes\n",
        "  vector = [0 for _ in range(30)]\n",
        "  # Create a vector for out-of-vocab words\n",
        "  oov = [0 for _ in range(30)]\n",
        "  \n",
        "  words = x.split(' ')\n",
        "  for word in words:\n",
        "    if word in model.wv.vocab:\n",
        "      # Word is present in the vocab\n",
        "      vector = np.sum([vector, model[word]], axis = 0)\n",
        "    else:\n",
        "      # Out of Vocabulary\n",
        "      vector = np.sum([vector, oov], axis = 0)\n",
        "\n",
        "  # Calculate the mean \n",
        "  mean_vector = vector / len(words)\n",
        "  X_train_vector_mean.append(mean_vector)\n"
      ],
      "metadata": {
        "id": "6glfLRaozZEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vector_appended = []\n",
        "max_words = 40\n",
        "for x in X_train:\n",
        "  words = x.split(' ')\n",
        "  num_words = max(max_words, len(words))\n",
        "  feature_vector = []\n",
        "  for word in words[:num_words]:\n",
        "    if word in model.wv.vocab:\n",
        "      # Word is present in the vocab\n",
        "      vector = np.sum([vector, model[word]], axis = 0)\n",
        "    else:\n",
        "      # Out of Vocabulary\n",
        "      vector = np.sum([vector, oov], axis = 0)\n",
        "    feature_vector = feature_vector + vector\n",
        "\n",
        "  if num_words < max_words:\n",
        "    pads = [0 for _ in range(30*(max_words-num_words))]\n",
        "    feature_vector = feature_vector + pads\n",
        "\n",
        "  X_train_vector_appended.append(feature_vector)\n"
      ],
      "metadata": {
        "id": "5L1B-PPvzZBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "model = MLPClassifier(hidden_layer_sizes = (1000, 700, 500, 200),\n",
        "                      max_iter = 100,\n",
        "                      activation = 'relu',\n",
        "                      solver = 'adam',\n",
        "                      random_state = 123)\n",
        "model.fit(X_train_vector_appended, Y_train)\n",
        "Y_predicted = model.predict(X_test_vector_appended)\n",
        "print(evaluate_model(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "k-OgPPbBzY-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_transformers import BertTokenizer\n",
        "from pytorch_transformers import BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base uncased',\n",
        "                                 output_hidden_states=True)\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "jfRNzajZz3Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_BERT = []\n",
        "for x in X_train:\n",
        "  # Add CLS and SEP\n",
        "  marked_text = \"[CLS] \" + x + \" [SEP]\"\n",
        "  # Split the sentence into tokens.\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  # Map the token strings to their vocabulary indeces.\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    feature_vector = outputs[0]\n",
        "\n",
        "  X_train_BERT.append(feature_vector)\n"
      ],
      "metadata": {
        "id": "fYaRL-nfz3Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_BERT = []\n",
        "for x in X_train:\n",
        "  # Add CLS and SEP\n",
        "  marked_text = \"[CLS] \" + x + \" [SEP]\"\n",
        "  # Split the sentence into tokens.\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  # Map the token strings to their vocabulary indeces.\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    hidden_states = outputs[2]\n",
        "    feature_vector = torch.stack(hidden_states).sum(0)\n",
        "  X_train_BERT.append(feature_vector)\n"
      ],
      "metadata": {
        "id": "kzSUUVj0z3Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_BERT = []\n",
        "for x in X_train:\n",
        "  # Add CLS and SEP\n",
        "  marked_text = \"[CLS] \" + x + \" [SEP]\"\n",
        "  # Split the sentence into tokens.\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  # Map the token strings to their vocabulary indeces.\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    hidden_states = outputs[2]\n",
        "    feature_vector = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)\n",
        "  X_train_BERT.append(feature_vector)\n"
      ],
      "metadata": {
        "id": "cP-HGgIc3XZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "model = MLPClassifier(hidden_layer_sizes = (1000, 700, 500, 200),\n",
        "                      max_iter = 100,\n",
        "                      activation = 'relu',\n",
        "                      solver = 'adam',\n",
        "                      random_state = 123)\n",
        "model.fit(X_train_BERT, Y_train)\n",
        "Y_predicted = model.predict(X_test_BERT)\n",
        "print(evaluate_model(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "id": "xRKWkB1U3dla"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}